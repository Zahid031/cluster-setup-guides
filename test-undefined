# Kubernetes 1.33 Cluster Setup Guide

This guide will help you set up a Kubernetes 1.34 cluster with Cilium networking using kubeadm.

## Prerequisites

- **OS**: Ubuntu 20.04+ or Debian 11+
- **Access**: Root or sudo privileges on all nodes
- **Hardware**: Minimum 2GB RAM, 2 CPU cores per node
- **Network**: All nodes can communicate with each other
- **Internet**: Connectivity required

---

## Part 1: Common Setup (All Nodes)

**⚠️ Important**: Run these steps on ALL nodes (master and worker nodes).

### Step 1: Set Hostname (Optional but Recommended)

```bash
# Set unique hostname for each node
sudo hostnamectl set-hostname master-node    # For master node
# sudo hostnamectl set-hostname worker-1     # For worker node 1
# sudo hostnamectl set-hostname worker-2     # For worker node 2

# Update hosts file (optional - for easier identification)
sudo nano /etc/hosts
# Add entries like:
# 192.168.1.10  master-node
# 192.168.1.11  worker-1
# 192.168.1.12  worker-2

# Verify hostname change
hostnamectl
```

### Step 2: Disable Swap

```bash
# Disable swap immediately
sudo swapoff -a

# Disable swap permanently
sudo sed -i '/swap/ s/^\(.*\)$/#\1/g' /etc/fstab

# Verify swap is disabled
free -h
```

### Step 3: Install Container Runtime (containerd)

```bash
# Update system packages
sudo apt update && sudo apt upgrade -y

# Install containerd
sudo apt install -y containerd

# Create containerd configuration
sudo mkdir -p /etc/containerd
containerd config default \
| sed 's/SystemdCgroup = false/SystemdCgroup = true/' \
| sed 's|sandbox_image = ".*"|sandbox_image = "registry.k8s.io/pause:3.10.1"|' \
| sudo tee /etc/containerd/config.toml > /dev/null

# Restart and enable containerd
sudo systemctl restart containerd
sudo systemctl enable containerd
```

### Step 4: Configure Kernel Modules and System Settings

```bash

# Make kernel modules persistent
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF

# Load required kernel modules
sudo modprobe overlay
sudo modprobe br_netfilter


# Configure sysctl parameters
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
EOF

# Apply sysctl settings
sudo sysctl --system

sudo sysctl -w net.ipv4.ip_forward=1
```

Verify IP forwarding:

```bash
sysctl net.ipv4.ip_forward
```
```

### Step 5: Add Kubernetes Repository

```bash
# Install required packages
sudo apt-get install -y apt-transport-https ca-certificates curl gpg

# Create directory for GPG keyrings
sudo mkdir -p /etc/apt/keyrings

# Add Kubernetes GPG key and repository

curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.34/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.34/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list

```

### Step 6: Install Kubernetes Components

```bash
# Update package index and install Kubernetes components
sudo apt update
sudo apt install -y kubelet kubeadm kubectl

# Prevent automatic updates
sudo apt-mark hold kubelet kubeadm kubectl

# Enable kubelet
sudo systemctl enable kubelet
```

---

## Part 2: Master Node Setup

**📍 Note**: Run these steps ONLY on the master node.

### Step 7: Initialize Kubernetes Cluster

```bash
# Find your master node IP address
ip addr show

# Initialize the cluster (replace with your actual master IP)
sudo kubeadm init \
  --apiserver-advertise-address=<YOUR_MASTER_IP> \
  --pod-network-cidr=192.168.0.0/16
```

**🔥 CRITICAL**: Save the `kubeadm join` command from the output!

### Step 8: Configure kubectl Access

```bash
# Setup kubectl configuration
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

## Step 9: Install Cilium (Master Node Only)

Cilium is used as the Container Network Interface (CNI) plugin for networking.

### Install Cilium CLI

Download and install the Cilium CLI.

```bash
CILIUM_CLI_VERSION=$(curl -s https://raw.githubusercontent.com/cilium/cilium-cli/main/stable.txt)
CLI_ARCH=amd64
if [ "$(uname -m)" = "aarch64" ]; then CLI_ARCH=arm64; fi
curl -L --fail --remote-name-all https://github.com/cilium/cilium-cli/releases/download/${CILIUM_CLI_VERSION}/cilium-linux-${CLI_ARCH}.tar.gz{,.sha256sum}
sha256sum --check cilium-linux-${CLI_ARCH}.tar.gz.sha256sum
sudo tar xzvfC cilium-linux-${CLI_ARCH}.tar.gz /usr/local/bin
rm cilium-linux-${CLI_ARCH}.tar.gz{,.sha256sum}
```

Verify the installation:

```bash
cilium version
```

### Install Cilium

Deploy Cilium version 1.17.4.

```bash
cilium install --version 1.17.4
cilium status
```




### Step 10: Allow Scheduling on Master (Optional)

```bash
# Remove taint to allow pod scheduling on master
kubectl taint nodes --all node-role.kubernetes.io/control-plane-
```

---



### Step 11: Join Worker Nodes

1. **Complete steps 1-6** (Common Setup) on each worker node first.

2. **Join the cluster** using the command from Step 7:

```bash
# Use the exact command from your master node initialization
sudo kubeadm join <MASTER_IP>:6443 --token <TOKEN> \
    --discovery-token-ca-cert-hash sha256:<HASH>
```

---

## Verification

```bash
# Check all nodes are ready
kubectl get nodes

# Check all system pods are running
kubectl get pods --all-namespaces

# Test with sample application
kubectl create deployment test-nginx --image=nginx
kubectl get pods

# Clean up test
kubectl delete deployment test-nginx
```

---

## Troubleshooting

### Token Expired
```bash
# Generate new join command (run on master)
kubeadm token create --print-join-command
```

### Node Not Ready
```bash
# Check node details and kubelet logs
kubectl describe node <node-name>
sudo journalctl -u kubelet -f
```

### Reset Cluster
```bash
# Reset kubeadm (run on each node)
sudo kubeadm reset -f
sudo rm -rf $HOME/.kube/config /etc/kubernetes/ /var/lib/etcd/
```

---

## Network Configuration

### Required Ports

**Master Node:** 6443, 2379-2380, 10250-10252
**Worker Nodes:** 10250, 30000-32767

### Network Settings
- **Pod Network CIDR**: `192.168.0.0/16`
- **Service Network**: `10.96.0.0/12`

---

## Quick Commands

```bash
kubectl get nodes                           # List nodes
kubectl get pods --all-namespaces         # List all pods
kubectl create deployment <n> --image=<i> # Create deployment
kubectl scale deployment <n> --replicas=3 # Scale deployment
kubectl delete deployment <n>             # Delete deployment
```
## Static IP

```bash
ip a show eth0          # Confirm current IP (192.168.121.62)
ip route | grep default # Note the gateway (e.g., 192.168.121.1)

sudo nano /etc/netplan/00-installer-config.yaml

network:
  version: 2
  renderer: networkd
  ethernets:
    eth0:
      dhcp4: no
      addresses: [192.168.121.62/24]   # Your current IP as static
      routes:
        - to: default
          via: 192.168.121.1           # Your gateway (from `ip route`)
      nameservers:
        addresses: [8.8.8.8, 8.8.4.4]  # Google DNS

sudo netplan --debug try
sudo netplan apply

ip a show eth0

```

---

## Part 3: Kubernetes Dashboard Setup with Helm

**📍 Note**: Run these steps on the master node after your cluster is fully operational.

### Step 12: Install Helm

```bash
# Download and install Helm
curl https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg > /dev/null
sudo apt-get install apt-transport-https --yes
echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list
sudo apt-get update
sudo apt-get install helm

# Verify Helm installation
helm version
```

### Step 13: Install Kubernetes Dashboard

```bash
# Add the Kubernetes Dashboard Helm repository
helm repo add kubernetes-dashboard https://kubernetes.github.io/dashboard/
helm repo update

# Create a namespace for the dashboard
kubectl create namespace kubernetes-dashboard

# Install the dashboard with Helm
helm install kubernetes-dashboard kubernetes-dashboard/kubernetes-dashboard \
    --namespace kubernetes-dashboard
```

### Step 14: Create Admin User and RBAC

```bash
# Create admin user and cluster role binding
kubectl apply -f - <<EOF
apiVersion: v1
kind: ServiceAccount
metadata:
  name: admin-user
  namespace: kubernetes-dashboard
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: admin-user
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: admin-user
  namespace: kubernetes-dashboard
EOF
```

### Step 15: Access the Dashboard

#### Method 1: Port Forward (Recommended for Quick Access)

```bash
# Port forward to access the dashboard
kubectl port-forward -n kubernetes-dashboard svc/kubernetes-dashboard-kong-proxy 9443:443
```

Open your browser and navigate to: `https://localhost:9443`

#### Method 2: NodePort Access (For Remote Access)

```bash
# Create a NodePort service for external access
kubectl apply -f - <<EOF
apiVersion: v1
kind: Service
metadata:
  name: dashboard-nodeport
  namespace: kubernetes-dashboard
spec:
  type: NodePort
  ports:
    - port: 443
      targetPort: 8443
      nodePort: 30443
      protocol: TCP
      name: https
  selector:
    app.kubernetes.io/name: kong
    app.kubernetes.io/instance: kubernetes-dashboard
EOF
```

Access the dashboard using any node IP: `https://<NODE_IP>:30443`

### Step 16: Get Access Token

```bash
# Generate access token for the admin user
kubectl -n kubernetes-dashboard create token admin-user
```

**💡 Important**: Copy and save this token - you'll need it to log into the dashboard.

### Step 17: Login to Dashboard

1. Open the dashboard URL in your browser
2. You'll see a certificate warning - click "Advanced" and "Proceed" (it's a self-signed certificate)
3. Select "Token" as the authentication method
4. Paste the token from Step 16
5. Click "Sign In"

### Dashboard Management Commands

```bash
# Check dashboard status
kubectl get pods -n kubernetes-dashboard
kubectl get svc -n kubernetes-dashboard

# View dashboard logs
kubectl logs -n kubernetes-dashboard deployment/kubernetes-dashboard-web

# Upgrade dashboard
helm repo update
helm upgrade kubernetes-dashboard kubernetes-dashboard/kubernetes-dashboard -n kubernetes-dashboard

# Uninstall dashboard (if needed)
helm uninstall kubernetes-dashboard -n kubernetes-dashboard
kubectl delete namespace kubernetes-dashboard
```

### Dashboard Features

Once logged in, you can:
- ✅ View cluster overview and resource usage
- ✅ Manage deployments, pods, and services
- ✅ View logs and execute commands in containers
- ✅ Monitor cluster health and events
- ✅ Create and manage Kubernetes resources via GUI

---

## Success!

Your Kubernetes 1.33 cluster is now ready with:

✅ Master and worker nodes configured
✅ Cilium networking enabled
✅ kubectl access configured
✅ Kubernetes Dashboard installed and accessible via Helm

Your cluster is ready for application deployment! 🚀

**Dashboard Access URLs:**
- Port Forward: `https://localhost:9443`
- NodePort: `https://<NODE_IP>:30443`

**Remember to save your admin-user token for dashboard access!**
