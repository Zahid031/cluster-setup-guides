# Kubernetes 1.34 Cluster Setup Guide

This guide will help you set up a Kubernetes 1.34 cluster with Cilium networking using kubeadm and configure the Kubernetes Dashboard with NodePort access.

## Prerequisites

- **OS**: Ubuntu 20.04+ or Debian 11+
- **Access**: Root or sudo privileges on all nodes
- **Hardware**: Minimum 2GB RAM, 2 CPU cores per node
- **Network**: All nodes can communicate with each other
- **Internet**: Connectivity required for package downloads

---

## Part 1: Common Setup (All Nodes)

**âš ï¸ Important**: Run these steps on ALL nodes (master and worker nodes).

### Step 1: Set Hostname (Optional but Recommended)

```bash
# Set unique hostname for each node
sudo hostnamectl set-hostname master-node    # For master node
# sudo hostnamectl set-hostname worker-1     # For worker node 1
# sudo hostnamectl set-hostname worker-2     # For worker node 2

# Update hosts file (optional - for easier identification)
sudo nano /etc/hosts
# Add entries like:
# 192.168.1.10  master-node
# 192.168.1.11  worker-1
# 192.168.1.12  worker-2

# Verify hostname change
hostnamectl
```

### Step 2: Disable Swap

```bash
# Disable swap immediately
sudo swapoff -a

# Disable swap permanently
sudo sed -i '/swap/ s/^\(.*\)$/#\1/g' /etc/fstab

# Verify swap is disabled
free -h
```

### Step 3: Install Container Runtime (containerd)

```bash
# Update system packages
sudo apt update && sudo apt upgrade -y

# Install containerd
sudo apt install -y containerd

# Create containerd configuration directory
sudo mkdir -p /etc/containerd

# Generate default containerd configuration with required modifications
containerd config default \
| sed 's/SystemdCgroup = false/SystemdCgroup = true/' \
| sed 's|sandbox_image = ".*"|sandbox_image = "registry.k8s.io/pause:3.10.1"|' \
| sudo tee /etc/containerd/config.toml > /dev/null

# Restart and enable containerd
sudo systemctl restart containerd
sudo systemctl enable containerd

# Verify containerd is running
sudo systemctl status containerd
```

### Step 4: Configure Kernel Modules and System Settings

```bash
# Create kernel modules configuration file
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF

# Load required kernel modules immediately
sudo modprobe overlay
sudo modprobe br_netfilter

# Configure sysctl parameters for Kubernetes
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
EOF

# Apply sysctl settings immediately
sudo sysctl --system

# Verify IP forwarding is enabled
sysctl net.ipv4.ip_forward
```

### Step 5: Add Kubernetes Repository

```bash
# Install required packages for repository management
sudo apt-get install -y apt-transport-https ca-certificates curl gpg

# Create directory for GPG keyrings
sudo mkdir -p /etc/apt/keyrings

# Add Kubernetes GPG key and repository for v1.34
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.34/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.34/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list
```

### Step 6: Install Kubernetes Components

```bash
# Update package index
sudo apt update

# Install Kubernetes components (kubelet, kubeadm, kubectl)
sudo apt install -y kubelet kubeadm kubectl

# Prevent automatic updates to maintain cluster stability
sudo apt-mark hold kubelet kubeadm kubectl

# Enable kubelet service
sudo systemctl enable kubelet

# Verify installation
kubeadm version
kubectl version --client
```

---

## Part 2: Master Node Setup

**ðŸ“ Note**: Run these steps ONLY on the master node.

### Step 7: Initialize Kubernetes Cluster

```bash
# Find your master node IP address
ip addr show

# Initialize the cluster (replace <YOUR_MASTER_IP> with your actual master IP)
sudo kubeadm init \
  --apiserver-advertise-address=<YOUR_MASTER_IP> \
  --pod-network-cidr=192.168.0.0/16 \
  --kubernetes-version=v1.34.0

# Example:
# sudo kubeadm init \
#   --apiserver-advertise-address=192.168.1.10 \
#   --pod-network-cidr=192.168.0.0/16 \
#   --kubernetes-version=v1.34.0
```

**ðŸ”¥ CRITICAL**: Save the `kubeadm join` command from the output! It will look like:
```
kubeadm join 192.168.1.10:6443 --token abc123.xyz789 \
    --discovery-token-ca-cert-hash sha256:1234567890abcdef...
```

### Step 8: Configure kubectl Access

```bash
# Setup kubectl configuration for the current user
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

# Verify kubectl access
kubectl get nodes
```

### Step 9: Install Cilium CNI Plugin

Cilium provides advanced networking capabilities for your Kubernetes cluster.

#### Install Cilium CLI

```bash
# Download and install Cilium CLI
CILIUM_CLI_VERSION=$(curl -s https://raw.githubusercontent.com/cilium/cilium-cli/main/stable.txt)
CLI_ARCH=amd64
if [ "$(uname -m)" = "aarch64" ]; then CLI_ARCH=arm64; fi

curl -L --fail --remote-name-all https://github.com/cilium/cilium-cli/releases/download/${CILIUM_CLI_VERSION}/cilium-linux-${CLI_ARCH}.tar.gz{,.sha256sum}
sha256sum --check cilium-linux-${CLI_ARCH}.tar.gz.sha256sum
sudo tar xzvfC cilium-linux-${CLI_ARCH}.tar.gz /usr/local/bin
rm cilium-linux-${CLI_ARCH}.tar.gz{,.sha256sum}

# Verify Cilium CLI installation
cilium version
```

#### Deploy Cilium

```bash
# Install Cilium with default configuration
cilium install

# Wait for Cilium to be ready
cilium status --wait

# Verify Cilium installation
kubectl get pods -n kube-system | grep cilium
```

### Step 10: Allow Scheduling on Master (Optional)

By default, master nodes don't schedule regular pods. Remove this restriction if desired:

```bash
# Remove taint to allow pod scheduling on master node
kubectl taint nodes --all node-role.kubernetes.io/control-plane-
```

---

## Part 3: Worker Node Setup

### Step 11: Join Worker Nodes

1. **Complete steps 1-6** (Common Setup) on each worker node first.

2. **Join the cluster** using the command saved from Step 7:

```bash
# Use the exact command from your master node initialization
sudo kubeadm join <MASTER_IP>:6443 --token <TOKEN> \
    --discovery-token-ca-cert-hash sha256:<HASH>

# Example:
# sudo kubeadm join 192.168.1.10:6443 --token abc123.xyz789 \
#     --discovery-token-ca-cert-hash sha256:1234567890abcdef...
```

3. **Verify nodes joined** (run on master):

```bash
kubectl get nodes
```

You should see all nodes in "Ready" status.

---

## Part 4: Kubernetes Dashboard Setup

**ðŸ“ Note**: Run these steps on the master node after your cluster is fully operational.

### Step 12: Install Helm Package Manager

```bash
# Download and install Helm
curl https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg > /dev/null
sudo apt-get install apt-transport-https --yes
echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list
sudo apt-get update
sudo apt-get install helm

# Verify Helm installation
helm version
```

### Step 13: Install Kubernetes Dashboard

```bash
# Add the Kubernetes Dashboard Helm repository
helm repo add kubernetes-dashboard https://kubernetes.github.io/dashboard/
helm repo update

# Create a dedicated namespace for the dashboard
kubectl create namespace kubernetes-dashboard

# Install the dashboard with Helm
helm install kubernetes-dashboard kubernetes-dashboard/kubernetes-dashboard \
    --namespace kubernetes-dashboard \
    --create-namespace
```

### Step 14: Configure Dashboard for NodePort Access

Instead of creating an additional service, we'll modify the existing Kong proxy service to use NodePort:

```bash
# Edit the Kong proxy service to change from ClusterIP to NodePort
kubectl patch service kubernetes-dashboard-kong-proxy \
    -n kubernetes-dashboard \
    -p '{"spec":{"type":"NodePort","ports":[{"port":443,"targetPort":8443,"nodePort":30443,"protocol":"TCP","name":"https"}]}}'

# Verify the service configuration
kubectl get svc kubernetes-dashboard-kong-proxy -n kubernetes-dashboard
```

### Step 15: Create Admin User and RBAC

```bash
# Create admin service account and cluster role binding
kubectl apply -f - <<EOF
apiVersion: v1
kind: ServiceAccount
metadata:
  name: admin-user
  namespace: kubernetes-dashboard
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: admin-user
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: admin-user
  namespace: kubernetes-dashboard
EOF
```

### Step 16: Get Access Token

```bash
# Generate access token for the admin user
kubectl -n kubernetes-dashboard create token admin-user

# For a long-lived token (optional), create a secret:
kubectl apply -f - <<EOF
apiVersion: v1
kind: Secret
metadata:
  name: admin-user-token
  namespace: kubernetes-dashboard
  annotations:
    kubernetes.io/service-account.name: admin-user
type: kubernetes.io/service-account-token
EOF

# Get the long-lived token:
kubectl get secret admin-user-token -n kubernetes-dashboard -o jsonpath='{.data.token}' | base64 -d
```

**ðŸ’¡ Important**: Copy and save this token - you'll need it to log into the dashboard.

### Step 17: Access the Dashboard

The dashboard is now accessible via NodePort on port 30443:

```bash
# Get your node IP addresses
kubectl get nodes -o wide

# Access the dashboard using any node IP
echo "Dashboard URL: https://<NODE_IP>:30443"
```

**Example**: If your master node IP is `192.168.1.10`, access: `https://192.168.1.10:30443`

### Step 18: Login to Dashboard

1. Open the dashboard URL in your browser: `https://<NODE_IP>:30443`
2. You'll see a certificate warning - click "Advanced" and "Proceed" (self-signed certificate)
3. Select "Token" as the authentication method
4. Paste the token from Step 16
5. Click "Sign In"

---

## Part 5: Network Configuration (Optional)

### Configure Static IP (If Required)

```bash
# Check current network configuration
ip a show eth0
ip route | grep default

# Edit netplan configuration
sudo nano /etc/netplan/00-installer-config.yaml

# Example configuration:
network:
  version: 2
  renderer: networkd
  ethernets:
    eth0:
      dhcp4: no
      addresses: [192.168.121.62/24]   # Your static IP
      routes:
        - to: default
          via: 192.168.121.1           # Your gateway
      nameservers:
        addresses: [8.8.8.8, 8.8.4.4]

# Apply the configuration
sudo netplan --debug try
sudo netplan apply

# Verify the static IP
ip a show eth0
```

### Required Network Ports

**Master Node:**
- 6443: Kubernetes API server
- 2379-2380: etcd server client API
- 10250: kubelet API
- 10251: kube-scheduler
- 10252: kube-controller-manager

**Worker Nodes:**
- 10250: kubelet API
- 30000-32767: NodePort services

**Dashboard:**
- 30443: Kubernetes Dashboard (NodePort)

---

## Verification and Testing

### Basic Cluster Verification

```bash
# Check all nodes are ready and in correct version
kubectl get nodes -o wide

# Check all system pods are running
kubectl get pods --all-namespaces

# Verify Cilium networking
cilium status
cilium connectivity test

# Check cluster info
kubectl cluster-info
```

### Test Application Deployment

```bash
# Create a test deployment
kubectl create deployment test-nginx --image=nginx:latest

# Expose the deployment
kubectl expose deployment test-nginx --port=80 --type=NodePort

# Check the deployment
kubectl get pods,svc

# Get the NodePort for testing
kubectl get svc test-nginx

# Clean up test resources
kubectl delete deployment test-nginx
kubectl delete service test-nginx
```

### Dashboard Verification

```bash
# Check dashboard pods status
kubectl get pods -n kubernetes-dashboard

# Check dashboard service configuration
kubectl get svc -n kubernetes-dashboard

# View dashboard logs (if needed)
kubectl logs -n kubernetes-dashboard deployment/kubernetes-dashboard-web
```

---

## Troubleshooting

### Common Issues and Solutions

#### Token Expired
```bash
# Generate new join command (run on master)
kubeadm token create --print-join-command
```

#### Node Not Ready
```bash
# Check node details
kubectl describe node <node-name>

# Check kubelet logs
sudo journalctl -u kubelet -f

# Check containerd status
sudo systemctl status containerd
```

#### Networking Issues
```bash
# Check Cilium status
cilium status

# Restart Cilium (if needed)
cilium restart

# Check pod network connectivity
kubectl exec -it <pod-name> -- ping <another-pod-ip>
```

#### Dashboard Access Issues
```bash
# Verify service is running
kubectl get svc kubernetes-dashboard-kong-proxy -n kubernetes-dashboard

# Check if NodePort is accessible
netstat -tlnp | grep :30443

# Verify firewall settings (if applicable)
sudo ufw status
```

### Reset Cluster (If Needed)

```bash
# Reset kubeadm on all nodes
sudo kubeadm reset -f

# Clean up configuration files
sudo rm -rf $HOME/.kube/config /etc/kubernetes/ /var/lib/etcd/

# Reset iptables (if needed)
sudo iptables -F && sudo iptables -t nat -F && sudo iptables -t mangle -F && sudo iptables -X
```

---

## Dashboard Management Commands

```bash
# View dashboard resources
kubectl get all -n kubernetes-dashboard

# Update dashboard (using Helm)
helm repo update
helm upgrade kubernetes-dashboard kubernetes-dashboard/kubernetes-dashboard -n kubernetes-dashboard

# Get dashboard service details
kubectl describe svc kubernetes-dashboard-kong-proxy -n kubernetes-dashboard

# View dashboard events
kubectl get events -n kubernetes-dashboard --sort-by='.lastTimestamp'

# Uninstall dashboard (if needed)
helm uninstall kubernetes-dashboard -n kubernetes-dashboard
kubectl delete namespace kubernetes-dashboard
```

---

## Quick Reference Commands

```bash
# Cluster Management
kubectl get nodes                              # List all nodes
kubectl get pods --all-namespaces            # List all pods
kubectl get services --all-namespaces        # List all services
kubectl cluster-info                          # Show cluster information

# Application Management
kubectl create deployment <name> --image=<image>    # Create deployment
kubectl expose deployment <name> --port=<port>      # Expose service
kubectl scale deployment <name> --replicas=<count>  # Scale deployment
kubectl delete deployment <name>                    # Delete deployment

# Troubleshooting
kubectl describe node <node-name>            # Node details
kubectl logs <pod-name>                      # Pod logs
kubectl exec -it <pod-name> -- /bin/bash    # Execute shell in pod
kubectl get events --sort-by='.lastTimestamp' # Recent events

# Dashboard
kubectl -n kubernetes-dashboard create token admin-user  # Get access token
kubectl get svc -n kubernetes-dashboard                  # Dashboard services
```

---

## Success! ðŸŽ‰

Your Kubernetes 1.34 cluster is now ready with:

âœ… **Master and worker nodes configured**  
âœ… **Cilium networking enabled**  
âœ… **kubectl access configured**  
âœ… **Kubernetes Dashboard accessible via NodePort**  
âœ… **Admin user with cluster-admin privileges**  

### Dashboard Access Information

- **URL**: `https://<NODE_IP>:30443`
- **Authentication**: Token-based
- **Admin Token**: Use the token generated in Step 16

### Next Steps

1. **Deploy your applications** using kubectl or the dashboard
2. **Configure ingress controllers** for external access
3. **Set up monitoring** with Prometheus/Grafana
4. **Implement backup strategies** for etcd data
5. **Configure resource quotas** and network policies

Your Kubernetes cluster is ready for production workloads! ðŸš€

---

## Additional Resources

- [Kubernetes Official Documentation](https://kubernetes.io/docs/)
- [Cilium Documentation](https://docs.cilium.io/)
- [Kubernetes Dashboard Documentation](https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/)
- [Helm Documentation](https://helm.sh/docs/)